# frozen_string_literal: false

# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in README.md and
#     CONTRIBUTING.md located at the root of this package.
#
# ----------------------------------------------------------------------------
require 'gcp_backend'
require 'google/dataproc/property/projectregionjob_driver_scheduling_config'
require 'google/dataproc/property/projectregionjob_flink_job'
require 'google/dataproc/property/projectregionjob_flink_job_logging_config'
require 'google/dataproc/property/projectregionjob_flink_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_flink_job_properties'
require 'google/dataproc/property/projectregionjob_hadoop_job'
require 'google/dataproc/property/projectregionjob_hadoop_job_logging_config'
require 'google/dataproc/property/projectregionjob_hadoop_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_hadoop_job_properties'
require 'google/dataproc/property/projectregionjob_hive_job'
require 'google/dataproc/property/projectregionjob_hive_job_properties'
require 'google/dataproc/property/projectregionjob_hive_job_query_list'
require 'google/dataproc/property/projectregionjob_hive_job_script_variables'
require 'google/dataproc/property/projectregionjob_labels'
require 'google/dataproc/property/projectregionjob_pig_job'
require 'google/dataproc/property/projectregionjob_pig_job_logging_config'
require 'google/dataproc/property/projectregionjob_pig_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_pig_job_properties'
require 'google/dataproc/property/projectregionjob_pig_job_query_list'
require 'google/dataproc/property/projectregionjob_pig_job_script_variables'
require 'google/dataproc/property/projectregionjob_placement'
require 'google/dataproc/property/projectregionjob_placement_cluster_labels'
require 'google/dataproc/property/projectregionjob_presto_job'
require 'google/dataproc/property/projectregionjob_presto_job_logging_config'
require 'google/dataproc/property/projectregionjob_presto_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_presto_job_properties'
require 'google/dataproc/property/projectregionjob_presto_job_query_list'
require 'google/dataproc/property/projectregionjob_pyspark_job'
require 'google/dataproc/property/projectregionjob_pyspark_job_logging_config'
require 'google/dataproc/property/projectregionjob_pyspark_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_pyspark_job_properties'
require 'google/dataproc/property/projectregionjob_reference'
require 'google/dataproc/property/projectregionjob_scheduling'
require 'google/dataproc/property/projectregionjob_spark_job'
require 'google/dataproc/property/projectregionjob_spark_job_logging_config'
require 'google/dataproc/property/projectregionjob_spark_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_spark_job_properties'
require 'google/dataproc/property/projectregionjob_spark_r_job'
require 'google/dataproc/property/projectregionjob_spark_r_job_logging_config'
require 'google/dataproc/property/projectregionjob_spark_r_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_spark_r_job_properties'
require 'google/dataproc/property/projectregionjob_spark_sql_job'
require 'google/dataproc/property/projectregionjob_spark_sql_job_logging_config'
require 'google/dataproc/property/projectregionjob_spark_sql_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_spark_sql_job_properties'
require 'google/dataproc/property/projectregionjob_spark_sql_job_query_list'
require 'google/dataproc/property/projectregionjob_spark_sql_job_script_variables'
require 'google/dataproc/property/projectregionjob_status'
require 'google/dataproc/property/projectregionjob_status_history'
require 'google/dataproc/property/projectregionjob_trino_job'
require 'google/dataproc/property/projectregionjob_trino_job_logging_config'
require 'google/dataproc/property/projectregionjob_trino_job_logging_config_driver_log_levels'
require 'google/dataproc/property/projectregionjob_trino_job_properties'
require 'google/dataproc/property/projectregionjob_trino_job_query_list'
require 'google/dataproc/property/projectregionjob_yarn_applications'

# A provider to manage Dataproc resources.
class DataprocProjectRegionJob < GcpResourceBase
  name 'google_dataproc_project_region_job'
  desc 'ProjectRegionJob'
  supports platform: 'gcp'

  attr_reader :params
  attr_reader :reference
  attr_reader :placement
  attr_reader :hadoop_job
  attr_reader :spark_job
  attr_reader :pyspark_job
  attr_reader :hive_job
  attr_reader :pig_job
  attr_reader :spark_r_job
  attr_reader :spark_sql_job
  attr_reader :presto_job
  attr_reader :trino_job
  attr_reader :flink_job
  attr_reader :status
  attr_reader :status_history
  attr_reader :yarn_applications
  attr_reader :driver_output_resource_uri
  attr_reader :driver_control_files_uri
  attr_reader :labels
  attr_reader :scheduling
  attr_reader :job_uuid
  attr_reader :done
  attr_reader :driver_scheduling_config

  def initialize(params)
    super(params.merge({ use_http_transport: true }))
    @params = params
    @fetched = @connection.fetch(product_url(params[:beta]), resource_base_url, params, 'Get')
    parse unless @fetched.nil?
  end

  def parse
    @reference = GoogleInSpec::Dataproc::Property::ProjectRegionJobReference.new(@fetched['reference'], to_s)
    @placement = GoogleInSpec::Dataproc::Property::ProjectRegionJobPlacement.new(@fetched['placement'], to_s)
    @hadoop_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobHadoopJob.new(@fetched['hadoopJob'], to_s)
    @spark_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobSparkJob.new(@fetched['sparkJob'], to_s)
    @pyspark_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobPysparkJob.new(@fetched['pysparkJob'], to_s)
    @hive_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobHiveJob.new(@fetched['hiveJob'], to_s)
    @pig_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobPigJob.new(@fetched['pigJob'], to_s)
    @spark_r_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobSparkRJob.new(@fetched['sparkRJob'], to_s)
    @spark_sql_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobSparkSqlJob.new(@fetched['sparkSqlJob'], to_s)
    @presto_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobPrestoJob.new(@fetched['prestoJob'], to_s)
    @trino_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobTrinoJob.new(@fetched['trinoJob'], to_s)
    @flink_job = GoogleInSpec::Dataproc::Property::ProjectRegionJobFlinkJob.new(@fetched['flinkJob'], to_s)
    @status = GoogleInSpec::Dataproc::Property::ProjectRegionJobStatus.new(@fetched['status'], to_s)
    @status_history = GoogleInSpec::Dataproc::Property::ProjectRegionJobStatusHistoryArray.parse(@fetched['statusHistory'], to_s)
    @yarn_applications = GoogleInSpec::Dataproc::Property::ProjectRegionJobYarnApplicationsArray.parse(@fetched['yarnApplications'], to_s)
    @driver_output_resource_uri = @fetched['driverOutputResourceUri']
    @driver_control_files_uri = @fetched['driverControlFilesUri']
    @labels = GoogleInSpec::Dataproc::Property::ProjectRegionJobLabels.new(@fetched['labels'], to_s)
    @scheduling = GoogleInSpec::Dataproc::Property::ProjectRegionJobScheduling.new(@fetched['scheduling'], to_s)
    @job_uuid = @fetched['jobUuid']
    @done = @fetched['done']
    @driver_scheduling_config = GoogleInSpec::Dataproc::Property::ProjectRegionJobDriverSchedulingConfig.new(@fetched['driverSchedulingConfig'], to_s)
  end

  def exists?
    !@fetched.nil?
  end

  def to_s
    "ProjectRegionJob #{@params[:jobId]}"
  end

  private

  def product_url(_ = nil)
    'https://dataproc.googleapis.com/v1/'
  end

  def resource_base_url
    'projects/{{project_id}}/regions/{{region}}/jobs/{{job_id}}'
  end
end
